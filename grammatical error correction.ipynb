{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvoxFu_1S4Vm"
   },
   "source": [
    "# Grammatical Error Correction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdDC0WXNS4W9"
   },
   "source": [
    "###  Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6708,
     "status": "ok",
     "timestamp": 1616914211695,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "WB_T68KOS4XA"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,RNN,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L83JBAgES4XF"
   },
   "source": [
    "###  Converting data from m2 to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MzGakOY-S4XG"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    this function stores the correct sentence line by line in as txt file with file name lang8.train.auto.bea19.m2\n",
    "    \"\"\"\n",
    "    \n",
    "    m2 = open(\"lang8.train.auto.bea19.m2\").read().strip().split(\"\\n\\n\")\n",
    "    out = open(\"lang8.train.auto.bea19.txt\", \"w\")\n",
    "    # Do not apply edits with these error types\n",
    "    skip = {\"noop\", \"UNK\", \"Um\"}\n",
    "    \n",
    "    for sent in m2:\n",
    "        sent = sent.split(\"\\n\")\n",
    "        cor_sent = sent[0].split()[1:] # Ignore \"S \"\n",
    "        edits = sent[1:]\n",
    "        offset = 0\n",
    "        for edit in edits:\n",
    "            edit = edit.split(\"|||\")\n",
    "            if edit[1] in skip: continue # Ignore certain edits\n",
    "            coder = int(edit[-1])\n",
    "            if coder != 0: continue # Ignore other coders\n",
    "            span = edit[0].split()[1:] # Ignore \"A \"\n",
    "            start = int(span[0])\n",
    "            end = int(span[1])\n",
    "            cor = edit[2].split()\n",
    "            cor_sent[start+offset:end+offset] = cor\n",
    "            offset = offset-(end-start)+len(cor)\n",
    "        out.write(\" \".join(cor_sent)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qg_zF1yNS4XI"
   },
   "outputs": [],
   "source": [
    "#preprocessing incorrect sentences\n",
    "\n",
    "file1 = open(\"lang8.train.auto.bea19.m2\",\"r\")\n",
    "s1 = file1.read()\n",
    "\n",
    "each_sent = s1.split(\"\\n\\n\")\n",
    "\n",
    "incorrect = []\n",
    "for i in range(len(each_sent)):\n",
    "    temp = each_sent[i].split(\"\\n\")\n",
    "    temp = temp[0]\n",
    "    temp = temp.split(\" \")\n",
    "    temp = temp[1:]# ignore S\n",
    "    temp = ' '.join(temp)\n",
    "    incorrect.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLCr5tyZS4XO"
   },
   "outputs": [],
   "source": [
    "#preprocessing correct sentences\n",
    "\n",
    "file2 = open(\"lang8.train.auto.bea19.txt\",\"r\")\n",
    "s2 = file2.read()\n",
    "\n",
    "correct = s2.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMX3XQYPS4XP"
   },
   "outputs": [],
   "source": [
    "# storing correct and incorrect sentence pair into dataframe\n",
    "df = pd.DataFrame()\n",
    "df[\"correct\"] = correct\n",
    "df[\"incorrect\"] = incorrect\n",
    "\n",
    "#store into csv file named data.csv\n",
    "df.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRhRiIBmS4XQ",
    "outputId": "591607c8-854e-4ebc-ee25-9ff94cc337ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good luck on your new start !</td>\n",
       "      <td>Good luck on your new start !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My teacher is going to move to change his job .</td>\n",
       "      <td>My teacher is going to move to change his job .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He is a so nice guy and taught me English very...</td>\n",
       "      <td>He is a so nice guy and taught me English very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And he took in my favorite subjects like soccer .</td>\n",
       "      <td>And he took in my favorite subject like soccer .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actually , he was the one who let me know abou...</td>\n",
       "      <td>Actually , who let me know about Lang - 8 was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             correct  \\\n",
       "0                      Good luck on your new start !   \n",
       "1    My teacher is going to move to change his job .   \n",
       "2  He is a so nice guy and taught me English very...   \n",
       "3  And he took in my favorite subjects like soccer .   \n",
       "4  Actually , he was the one who let me know abou...   \n",
       "\n",
       "                                           incorrect  \n",
       "0                      Good luck on your new start !  \n",
       "1    My teacher is going to move to change his job .  \n",
       "2  He is a so nice guy and taught me English very...  \n",
       "3   And he took in my favorite subject like soccer .  \n",
       "4  Actually , who let me know about Lang - 8 was ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4pm8vH1S4XZ",
    "outputId": "c87de21d-9712-4322-c943-6443981f6571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037562, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmWOyh9LS4Xc"
   },
   "source": [
    "We can see that there are 1037562 pair of correct and incorrect sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekh5OP-KS4Xe"
   },
   "source": [
    "###  Analysing data and doing some simple data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHrIOg3BS4Xg",
    "outputId": "3dfb334a-265c-43b0-c97d-819405499cf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498362, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = []\n",
    "for i in range(len(df.values)):\n",
    "    if df.values[i][0] == df.values[i][1]:\n",
    "        index.append(i)\n",
    "            \n",
    "df = df.drop(index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSPzMWWS4Xh"
   },
   "source": [
    "We can see that out of 1037562 data points only 498362 are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8OAEv2ES4Xi"
   },
   "outputs": [],
   "source": [
    "## save this new dataset to disk\n",
    "df.to_csv(\"new_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BqU9Hc6XS4Xn",
    "outputId": "268fcc9d-9160-4060-f53c-682d388c271a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And he took in my favorite subjects like soccer .</td>\n",
       "      <td>And he took in my favorite subject like soccer .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actually , he was the one who let me know abou...</td>\n",
       "      <td>Actually , who let me know about Lang - 8 was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             correct  \\\n",
       "0  And he took in my favorite subjects like soccer .   \n",
       "1  Actually , he was the one who let me know abou...   \n",
       "\n",
       "                                           incorrect  \n",
       "0   And he took in my favorite subject like soccer .  \n",
       "1  Actually , who let me know about Lang - 8 was ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"new_data.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Wx42adS4Xp"
   },
   "source": [
    "### Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZfyTyFp8S4Xp",
    "outputId": "ecbc2fe0-b314-4c27-9380-10ec603acfee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckeck if any missing value is present\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOHdr0syS4Xq"
   },
   "source": [
    "We can see that there are null values in pyton, So we will dop the rows containing null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HXPTXDspS4Xr",
    "outputId": "da188ca9-a984-4b36-91c2-7c6e6a44c648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498360, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5tWAMEKS4Xs"
   },
   "source": [
    "There were only two rows with null valued which have been deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8575_37S4Xt"
   },
   "source": [
    "###  Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uNBbVw0AS4Xu",
    "outputId": "4ea6ef10-d788-493c-af4b-5dd49ccf862b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckeck if any duplicate value is present\n",
    "df.duplicated().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zp3mw_0PS4Xv",
    "outputId": "46435551-ee06-4e5f-be60-9fe46baa882f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496339, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NtNkgd8VS4Xx",
    "outputId": "dd7bd282-e525-4fcc-8d13-8315dc076418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht4thUNFS4X1"
   },
   "source": [
    "There were 498360 - 496339 = 2021 duplicate rows which have been removed and only the first occurance of these rows ia kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_f897QmkS4X3"
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zwQKSngQS4X4"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    takes string as input and\n",
    "    removes characters inside (),{},[] and <>\n",
    "    removes characters like -+@#^/|*(){}$~`\n",
    "    we not not removing ,.!-:;\"' as these characters are present in english language \n",
    "    \"\"\"\n",
    "    text = re.sub('<.*>', '', text)\n",
    "    text = re.sub('\\(.*\\)', '', text)\n",
    "    text = re.sub('\\[.*\\]', '', text)\n",
    "    text = re.sub('{.*}', '', text)\n",
    "    text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = re.sub(\"\\[\",\"\",text)\n",
    "    text = re.sub(\"\\]\",\"\",text)\n",
    "    text = re.sub(\"[0-9]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "df[\"correct\"] = df[\"correct\"].apply(clean)\n",
    "df[\"incorrect\"] = df[\"incorrect\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R-rUejrTS4X4",
    "outputId": "100288ed-5f10-439d-d7ee-29ce961d3e9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9C_75AmS4X8"
   },
   "source": [
    "###  Analyse length of correct sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VFEcqw9uS4X9"
   },
   "outputs": [],
   "source": [
    "def percentile(low,high,step,list_temp):\n",
    "    \"\"\"\n",
    "    this function takes low, high, step size as input and prints percentiles accordingly\n",
    "    \"\"\"\n",
    "    for i in np.arange(low,high,step):\n",
    "        print(i,\"percentile is \",np.percentile(list_temp ,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c2Ru6sfS4X-",
    "outputId": "23f6af96-99db-42c8-9df2-1b0b0a3bd7be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496339"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sen_to_char(sen):\n",
    "    return len([i for i in sen])\n",
    "\n",
    "corr_length = df[\"correct\"].apply(sen_to_char)\n",
    "corr_length = list(corr_length)\n",
    "len(corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZoO4lxDS4YB",
    "outputId": "308383e9-8c40-4b44-caf9-f013b26d77d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  27.0\n",
      "20 percentile is  35.0\n",
      "30 percentile is  42.0\n",
      "40 percentile is  48.0\n",
      "50 percentile is  55.0\n",
      "60 percentile is  63.0\n",
      "70 percentile is  73.0\n",
      "80 percentile is  86.0\n",
      "90 percentile is  108.0\n",
      "100 percentile is  2622.0\n",
      "***************************************************************\n",
      "90 percentile is  108.0\n",
      "91 percentile is  112.0\n",
      "92 percentile is  116.0\n",
      "93 percentile is  120.0\n",
      "94 percentile is  125.0\n",
      "95 percentile is  131.0\n",
      "96 percentile is  139.0\n",
      "97 percentile is  149.0\n",
      "98 percentile is  163.0\n",
      "99 percentile is  188.0\n",
      "100 percentile is  2622.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,corr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3-5Od9ZS4YC",
    "outputId": "93176b73-7b9a-48f5-d3d1-f6d865ccaa41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446785, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have correct sentence of length more than 108\n",
    "index = []\n",
    "for i in range(len(corr_length)):\n",
    "    if corr_length[i] > 108:\n",
    "        index.append(i)\n",
    "        \n",
    "df.drop(index,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt-GU_IuS4YF",
    "outputId": "34b3009d-d561-4405-a1dd-e16ceb1c07fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOIl9W5DS4YH"
   },
   "source": [
    "For incorrect sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Cbcu4VbS4YH",
    "outputId": "62b68a95-6f5a-4f67-e43f-f2962803a737"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446785"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorr_length = df[\"incorrect\"].apply(sen_to_char)\n",
    "incorr_length = list(incorr_length)\n",
    "len(incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SfPmC2_S4YI",
    "outputId": "8942b671-44e3-4f30-c2c1-f5f37ee618c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  25.0\n",
      "20 percentile is  32.0\n",
      "30 percentile is  38.0\n",
      "40 percentile is  44.0\n",
      "50 percentile is  50.0\n",
      "60 percentile is  57.0\n",
      "70 percentile is  64.0\n",
      "80 percentile is  73.0\n",
      "90 percentile is  85.0\n",
      "100 percentile is  611.0\n",
      "***********************************************************\n",
      "90 percentile is  85.0\n",
      "91 percentile is  87.0\n",
      "92 percentile is  89.0\n",
      "93 percentile is  91.0\n",
      "94 percentile is  92.0\n",
      "95 percentile is  95.0\n",
      "96 percentile is  97.0\n",
      "97 percentile is  99.0\n",
      "98 percentile is  103.0\n",
      "99 percentile is  107.0\n",
      "100 percentile is  611.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,incorr_length)\n",
    "print(\"***********************************************************\")\n",
    "percentile(90,101,1,incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB46IyG3S4YM",
    "outputId": "164796a9-45a4-40c3-c435-0b649e2b32a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443397, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have incorrect sentence of length more than 108\n",
    "index = []\n",
    "for i in range(len(incorr_length)):\n",
    "    if incorr_length[i] > 108:\n",
    "        index.append(i)\n",
    "        \n",
    "df.drop(index,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_length = df[\"correct\"].str.split().apply(len)\n",
    "corr_length = list(corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  6.0\n",
      "20 percentile is  8.0\n",
      "30 percentile is  9.0\n",
      "40 percentile is  11.0\n",
      "50 percentile is  12.0\n",
      "60 percentile is  14.0\n",
      "70 percentile is  16.0\n",
      "80 percentile is  18.0\n",
      "90 percentile is  23.0\n",
      "100 percentile is  488.0\n",
      "***************************************************************\n",
      "90 percentile is  23.0\n",
      "91 percentile is  23.0\n",
      "92 percentile is  24.0\n",
      "93 percentile is  25.0\n",
      "94 percentile is  26.0\n",
      "95 percentile is  27.0\n",
      "96 percentile is  28.0\n",
      "97 percentile is  30.0\n",
      "98 percentile is  33.0\n",
      "99 percentile is  38.0\n",
      "100 percentile is  488.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,corr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(459812, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have correct sentence of length more than 24\n",
    "index = []\n",
    "for i in range(len(corr_length)):\n",
    "    if corr_length[i] > 24:\n",
    "        index.append(i)\n",
    "        \n",
    "df.drop(index,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorr_length = df[\"incorrect\"].str.split().apply(len)\n",
    "incorr_length = list(incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  6.0\n",
      "20 percentile is  7.0\n",
      "30 percentile is  9.0\n",
      "40 percentile is  10.0\n",
      "50 percentile is  11.0\n",
      "60 percentile is  12.0\n",
      "70 percentile is  14.0\n",
      "80 percentile is  16.0\n",
      "90 percentile is  19.0\n",
      "100 percentile is  128.0\n",
      "***************************************************************\n",
      "90 percentile is  19.0\n",
      "91 percentile is  19.0\n",
      "92 percentile is  20.0\n",
      "93 percentile is  20.0\n",
      "94 percentile is  20.0\n",
      "95 percentile is  21.0\n",
      "96 percentile is  22.0\n",
      "97 percentile is  22.0\n",
      "98 percentile is  23.0\n",
      "99 percentile is  24.0\n",
      "100 percentile is  128.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,incorr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458206, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have incorrect sentence of length more than 25\n",
    "index = []\n",
    "for i in range(len(incorr_length)):\n",
    "    if incorr_length[i] > 25:\n",
    "        index.append(i)\n",
    "        \n",
    "df.drop(index,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l30jfm-0S4YP"
   },
   "source": [
    "###  Splitting data into train, CV and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c9dZVwt4S4YQ"
   },
   "outputs": [],
   "source": [
    "train_temp,cv = train_test_split(df, test_size=0.1)\n",
    "train,test = train_test_split(train_temp, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "GSrX5M1NS4YR"
   },
   "outputs": [],
   "source": [
    "#store dataset into disk after splitting\n",
    "train.to_csv(\"train_word.csv\",index=False)\n",
    "cv.to_csv(\"cv_word.csv\",index=False)\n",
    "test.to_csv(\"test_word.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cheVqOXaS4YS"
   },
   "source": [
    "###  Unique words in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1616914293244,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "I2rVW2XIS4YS"
   },
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "train = pd.read_csv(\"train_word.csv\")\n",
    "cv = pd.read_csv(\"cv_word.csv\")\n",
    "test = pd.read_csv(\"test_word.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLCCG34DS4YZ"
   },
   "source": [
    "Unique words in Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JqU1Xj-sS4Ya",
    "outputId": "ece2efbd-d4a0-49d7-e8b4-22d97d06641c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in incorrect sentences in train data are 87564\n"
     ]
    }
   ],
   "source": [
    "# for incorrect sentences\n",
    "split_sent = np.array(train[\"incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    if type(i) == float:\n",
    "        continue\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_train_incorr = set(unique)\n",
    "print(\"total number of unique words in incorrect sentences in train data are\",len(unique_words_train_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1o6BA4ZwS4Yb",
    "outputId": "126a52ca-1a97-4fd8-f4c4-5bbf1046e808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in correct sentences in train are 68105\n"
     ]
    }
   ],
   "source": [
    "# for correct sentences\n",
    "split_sent = np.array(train[\"correct\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_train_corr = set(unique)\n",
    "print(\"total number of unique words in correct sentences in train are\",len(unique_words_train_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zAL_fZnS4Yc"
   },
   "source": [
    "Unique words in CV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "i05m4va1S4Yd",
    "outputId": "6fb873af-c040-4727-f868-450e1c8f166e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in incorrect sentences in cv data are 6300\n"
     ]
    }
   ],
   "source": [
    "# for incorrect sentences\n",
    "split_sent = np.array(cv[\"incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_cv_incorr = set(unique)\n",
    "print(\"total number of unique words in incorrect sentences in cv data are\",len(unique_words_cv_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NE2SOsDDS4Ye",
    "outputId": "0dd6d03b-e303-40c2-e7d9-a4429a95231c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in correct sentences in cv data are 5959\n"
     ]
    }
   ],
   "source": [
    "# for correct sentences\n",
    "split_sent = np.array(cv[\"correct\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_cv_corr = set(unique)\n",
    "print(\"total number of unique words in correct sentences in cv data are\",len(unique_words_cv_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXLD3IDhS4Yp"
   },
   "source": [
    "Unique words in Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sCuaSyVCS4Yp",
    "outputId": "af974034-3081-4f1b-c0bc-c01a4e387f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in incorrect sentences in test data are 6300\n"
     ]
    }
   ],
   "source": [
    "# for incorrect sentences\n",
    "split_sent = np.array(test[\"incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_test_incorr = set(unique)\n",
    "print(\"total number of unique words in incorrect sentences in test data are\",len(unique_words_cv_incorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "EG4E1yY7S4Yr",
    "outputId": "b8dc41b2-47ac-4e87-8f24-ebbb75a370a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in correct sentences in test are 6349\n"
     ]
    }
   ],
   "source": [
    "split_sent = np.array(test[\"incorrect\"].str.split())\n",
    "\n",
    "unique = []\n",
    "for i in split_sent:\n",
    "    for j in i:\n",
    "        unique.append(j)\n",
    "\n",
    "unique_words_test_corr = set(unique)\n",
    "print(\"total number of unique words in correct sentences in test are\",len(unique_words_test_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNqy8hCFS4Yt"
   },
   "source": [
    "##  Making data model ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D7eMlDR4mmcr"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "cv = pd.read_csv(\"cv.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add $ to each sentence which will be input to decoder We will add @ to each sentence which will be output of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1616914316973,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "B0kUMv5zS4Yu",
    "outputId": "e7d5543f-8017-4837-b3c8-d41b80cac307"
   },
   "outputs": [],
   "source": [
    "train[\"correct_inp\"] = \"$\" + train[\"correct\"].astype(str)#$ denotes start of sentence\n",
    "train[\"correct_out\"] = train[\"correct\"].astype(str) + \"@\"#@ denotes end of sentence\n",
    "\n",
    "cv[\"correct_inp\"] = \"$\" + cv[\"correct\"].astype(str)\n",
    "cv[\"correct_out\"] = cv[\"correct\"].astype(str) + \"@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1616914320085,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "IN1LdtYOS4Yv"
   },
   "outputs": [],
   "source": [
    "# loading saved tokenizer\n",
    "with open(\"tokenizer_incorr.pickle\",\"rb\") as temp1:\n",
    "    tokenizer_incorr = pickle.load(temp1)\n",
    "    \n",
    "with open(\"tokenizer_corr_inp.pickle\",\"rb\") as temp2:\n",
    "    tokenizer_corr_inp = pickle.load(temp2)\n",
    "    \n",
    "with open(\"tokenizer_corr_out.pickle\",\"rb\") as temp3:\n",
    "    tokenizer_corr_out = pickle.load(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing senetence for feeding to encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14103,
     "status": "ok",
     "timestamp": 1616914336439,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "OOo65fHRS4Yx",
    "outputId": "9bfcaa90-091f-417b-b95e-d8e24d62c4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size of incorrrect sentences is 62\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_incorr = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "#tokenizer_incorr.fit_on_texts(train[\"incorrect\"].values)\n",
    "\n",
    "incorr_train = np.array(tokenizer_incorr.texts_to_sequences(train[\"incorrect\"].values))\n",
    "incorr_cv = np.array(tokenizer_incorr.texts_to_sequences(cv[\"incorrect\"].values))\n",
    "print(\"vocab size of incorrrect sentences is\",len(tokenizer_incorr.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing senetence for feeding to decoder as inpput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14728,
     "status": "ok",
     "timestamp": 1616914351192,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "zofM0u0qS4Y3",
    "outputId": "65a5be8f-d35c-43cb-f64e-e8f7569ed44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size of corrrect sentences is 63\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_corr_inp = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "#tokenizer_corr_inp.fit_on_texts(train[\"correct_inp\"].values)\n",
    "\n",
    "corr_train_inp = np.array(tokenizer_corr_inp.texts_to_sequences(train[\"correct_inp\"].values))\n",
    "corr_cv_inp = np.array(tokenizer_corr_inp.texts_to_sequences(cv[\"correct_inp\"].values))\n",
    "print(\"vocab size of corrrect sentences is\",len(tokenizer_corr_inp.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing senetence which will be output of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 27641,
     "status": "ok",
     "timestamp": 1616914364113,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "x76l7O5SS4Y5"
   },
   "outputs": [],
   "source": [
    "#tokenizer_corr_out = Tokenizer(filters=\"\",char_level=True,lower=False)\n",
    "#tokenizer_corr_out.fit_on_texts(train[\"correct_out\"].values)\n",
    "\n",
    "corr_train_out = np.array(tokenizer_corr_out.texts_to_sequences(train[\"correct_out\"].values))\n",
    "corr_cv_out = np.array(tokenizer_corr_inp.texts_to_sequences(cv[\"correct_out\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding train, cv, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 38907,
     "status": "ok",
     "timestamp": 1616914375393,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "ikvPdY4sS4Y7"
   },
   "outputs": [],
   "source": [
    "incorr_train = np.array(pad_sequences(incorr_train,maxlen=110,padding=\"post\",truncating='post'))\n",
    "corr_train_inp = np.array(pad_sequences(corr_train_inp,maxlen=110,padding=\"post\",truncating='post'))\n",
    "corr_train_out = np.array(pad_sequences(corr_train_out,maxlen=110,padding=\"post\",truncating='post'))\n",
    "\n",
    "incorr_cv = np.array(pad_sequences(incorr_cv,maxlen=110,padding=\"post\",truncating='post'))\n",
    "corr_cv_inp = np.array(pad_sequences(corr_cv_inp,maxlen=110,padding=\"post\",truncating='post'))\n",
    "corr_cv_out = np.array(pad_sequences(corr_cv_out,maxlen=110,padding=\"post\",truncating='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DKEU19YJS4Y9"
   },
   "outputs": [],
   "source": [
    "##save keras tokenizer\n",
    "\n",
    "with open(\"tokenizer_incorr.pickle\",\"wb\") as temp1:\n",
    "    pickle.dump(tokenizer_incorr,temp1)\n",
    "    \n",
    "with open(\"tokenizer_corr_inp.pickle\",\"wb\") as temp2:\n",
    "    pickle.dump(tokenizer_corr_inp,temp2)\n",
    "    \n",
    "with open(\"tokenizer_corr_out.pickle\",\"wb\") as temp3:\n",
    "    pickle.dump(tokenizer_corr_out,temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ILS5-ReS4Y-"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w7k1LNJS4Y_"
   },
   "source": [
    "We will use a encoder decoder model as a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 38898,
     "status": "ok",
     "timestamp": 1616914375396,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "2ww-x5eLS4Y_"
   },
   "outputs": [],
   "source": [
    "# code taken from attention mechanism assignment\n",
    "\n",
    "############################## Encoder class #############################################################\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\")\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "        self.encoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "\n",
    "        output1 = self.embedding(input_sequence)\n",
    "        enco_output, enco_state_h, enco_state_c = self.encoder_lstm(output1, initial_state=states)\n",
    "        return enco_output, enco_state_h, enco_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "\n",
    "        initial_hidden_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        initial_cell_state = tf.zeros([batch_size,self.lstm_size])\n",
    "        \n",
    "        return [initial_hidden_state,initial_cell_state]\n",
    "\n",
    "############################## Decoder class #############################################################\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = Embedding(input_dim=out_vocab_size, output_dim=embedding_size, input_length=input_length,\n",
    "                           mask_zero=True,name=\"embedding_layer_encoder\")\n",
    "        self.lstmcell = tf.keras.layers.LSTMCell(lstm_size)\n",
    "        self.decoder_lstm = RNN(self.lstmcell,return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self,target_sequence,initial_states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
    "        \n",
    "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
    "        '''\n",
    "        output2 = self.embedding(target_sequence)\n",
    "        deco_output, deco_state_h, deco_state_c = self.decoder_lstm(output2, initial_state=initial_states)\n",
    "      \n",
    "        return deco_output, deco_state_h, deco_state_c\n",
    "\n",
    "##############################encoder decoder class#############################################################    \n",
    "    \n",
    "qw_state = 0\n",
    "class Encoder_decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,inp_vocab_size,out_vocab_size,embedding_size,lstm_size,input_length,batch_size,*args):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size,embedding_size,lstm_size,input_length)\n",
    "        #print(\"output vocab size in encoder decoder class\",out_vocab_size)\n",
    "        self.decoder = Decoder(out_vocab_size,embedding_size,lstm_size,input_length)\n",
    "        self.dense   = Dense(out_vocab_size)#, activation='softmax')\n",
    "        self.batch = batch_size\n",
    "    \n",
    "    \n",
    "    def call(self,data,*args):\n",
    "        '''\n",
    "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        C. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        \n",
    "        input,output = data[0], data[1]\n",
    "        # initializing initial states of encoder\n",
    "        l = self.encoder.initialize_states(self.batch)\n",
    "        qw_state = l\n",
    "        #print(\"WE ARE INITIALIZING encoder WITH initial STATES as zeroes :\",l[0].shape, l[1].shape)\n",
    "        #print(\"hello\")\n",
    "        encoder_output,encoder_final_state_h,encoder_final_state_c = self.encoder(input,l)\n",
    "        #print(\"ENCODER ==> OUTPUT SHAPE\",encoder_output.shape)\n",
    "        #print(\"ENCODER ==> HIDDEN STATE SHAPE\",encoder_final_state_h.shape)\n",
    "        #print(\"ENCODER ==> CELL STATE SHAPE\", encoder_final_state_c.shape)\n",
    "        #print(\"hi\")\n",
    "        m = list((encoder_final_state_h,encoder_final_state_c))\n",
    "        decoder_output,decoder_final_state_h,decoder_final_state_c = self.decoder(output,m)\n",
    "        #print(\"decoder OUTPUT SHAPE\",decoder_output.shape)\n",
    "        #print(\"type of decoder output is \",type(decoder_output))\n",
    "        #x = self.flatten(decoder_output)\n",
    "        #print(\"shape of x \",x.shape)\n",
    "        qw_output = self.dense(decoder_output)\n",
    "        #print(\"FINAL OUTPUT SHAPE\",qw_output.shape)\n",
    "        return qw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder Model with character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 28896,
     "status": "ok",
     "timestamp": 1616914380266,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "7ufcA7HMS4ZB"
   },
   "outputs": [],
   "source": [
    "inp_vocab_size = 63\n",
    "out_vocab_size = 64\n",
    "embedding_dim=100\n",
    "input_length=110\n",
    "lstm_size=256\n",
    "batch_size=1024\n",
    "#model = Encoder_decoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size)\n",
    "# custom loss function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "#defining custom loss function which will not consider loss for padded zeroes\n",
    "# code taken from attention assignment\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "#model.compile(optimizer=optimizer,loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the TensorBoard notebook extension\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4683990,
     "status": "ok",
     "timestamp": 1616919129776,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "gQ8S5sOWS4ZK",
    "outputId": "f8d89206-069a-4c32-edfd-23f38a60095f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "384/384 [==============================] - 194s 492ms/step - loss: 0.6976\n",
      "Epoch 2/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.4391\n",
      "Epoch 3/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.3616\n",
      "Epoch 4/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.3217\n",
      "Epoch 5/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2987\n",
      "Epoch 6/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.2815\n",
      "Epoch 7/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2652\n",
      "Epoch 8/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.2514\n",
      "Epoch 9/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2391\n",
      "Epoch 10/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2288\n",
      "Epoch 11/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2210\n",
      "Epoch 12/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.2143\n",
      "Epoch 13/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.2083\n",
      "Epoch 14/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.2035\n",
      "Epoch 15/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.1991\n",
      "Epoch 16/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.1951\n",
      "Epoch 17/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.1923\n",
      "Epoch 18/25\n",
      "384/384 [==============================] - 187s 486ms/step - loss: 0.1890\n",
      "Epoch 19/25\n",
      "384/384 [==============================] - 187s 487ms/step - loss: 0.1865\n",
      "Epoch 20/25\n",
      "384/384 [==============================] - 187s 488ms/step - loss: 0.1842\n",
      "Epoch 21/25\n",
      "384/384 [==============================] - 188s 488ms/step - loss: 0.1815\n",
      "Epoch 22/25\n",
      "384/384 [==============================] - 187s 488ms/step - loss: 0.1796\n",
      "Epoch 23/25\n",
      "384/384 [==============================] - 187s 488ms/step - loss: 0.1774\n",
      "Epoch 24/25\n",
      "384/384 [==============================] - 187s 488ms/step - loss: 0.1764\n",
      "Epoch 25/25\n",
      "384/384 [==============================] - 187s 488ms/step - loss: 0.1751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe72876350>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model.fit(x=[incorr_train,corr_train_inp],y=corr_train_out, epochs=10,batch_size=1024,callbacks=[tensorboard_callback],validation_data=([incorr_train_cv,corr_cv_inp],corr_cv_out))\n",
    "#model.fit(x=[temp1,temp2],y=temp3, epochs=1,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1753,
     "status": "ok",
     "timestamp": 1616919133128,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "oKd7n7eanRoE",
    "outputId": "174052c7-1686-4403-c438-39be84f7dbe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  371868    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  371968    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  16448     \n",
      "=================================================================\n",
      "Total params: 760,284\n",
      "Trainable params: 760,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2140,
     "status": "ok",
     "timestamp": 1616919975240,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "26cDvqduS4ZL"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c197a8c688>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code taken from https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=mJqOn0snzCRy\n",
    "#model.save_weights('enco_dec_char', save_format='tf')\n",
    "\n",
    "model = Encoder_decoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size)\n",
    "model.compile(optimizer=optimizer,loss=loss_function)\n",
    "model.train_on_batch([incorr_train[:1024],corr_train_inp[:1024]],corr_train_out[:1024])\n",
    "# Load the state of the old model\n",
    "model.load_weights('enco_dec_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code taken from attention mechanism assignment\n",
    "corr_dict = tokenizer_corr_out.word_index\n",
    "inv_corr = {v: k for k, v in corr_dict.items()}\n",
    "\n",
    "def predict(input_sentence):\n",
    "    \"\"\"\n",
    "    this function takes incorrect input sentences s input and retirns correct sentences\n",
    "    \"\"\"\n",
    "    input_sentence = tokenizer_incorr.texts_to_sequences([input_sentence])\n",
    "    initial_hidden_state = tf.zeros([1,256])\n",
    "    initial_cell_state = tf.zeros([1,256])\n",
    "    qwst = [initial_hidden_state,initial_cell_state]\n",
    "    pred_total = []\n",
    "    enc_output, enc_state_h, enc_state_c = model.layers[0](np.expand_dims(input_sentence[0],0),qwst)\n",
    "    states_values = [enc_state_h, enc_state_c]\n",
    "    pred = []\n",
    "    sentence = []\n",
    "    cur_vec = np.array([[16]])#np.ones((1, 1),dtype='int')\n",
    "    for i in range(110):\n",
    "        dec_output, dec_state_h, dec_state_c = model.layers[1](cur_vec,states_values)\n",
    "        infe_output=model.layers[2](dec_output)\n",
    "        states_values = [dec_state_h, dec_state_c]\n",
    "        cur_vec = np.reshape(np.argmax(infe_output), (1, 1))\n",
    "        if inv_corr[cur_vec[0][0]] == '@':\n",
    "            break\n",
    "            #print(\"at time step \",i,\" the word is \", cur_vec)\n",
    "        pred.append(cur_vec[0][0])\n",
    "    for i in pred:\n",
    "        sentence.append(inv_corr[i])\n",
    "    #return pred\n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediting results on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love very much , which is one of the Dariso .\n",
      "Sell Gimera , and so on .\n",
      "Nowadays I have to find the student for a restaurant to learn .\n",
      "It is one of the most famous and many .\n",
      "I would like to learn English , but I will try to learn .\n",
      "Just this writing this website .\n",
      "Then I put them on the other .\n",
      "The weather is nice but we are planning to move to the station .\n",
      "I 'm option and I could n't go to the United early , I feel happy .\n",
      "She never been in a box , I had a content .\n"
     ]
    }
   ],
   "source": [
    "#predicted sentences\n",
    "for i in train[\"incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I love it very much when I am in Odessa . It 's raining .\",\n",
       "       'Selena Gomez sings it .',\n",
       "       'Nowadays I take a fresh interest in studying a foreign language .',\n",
       "       'It is one of my most favorite manga .',\n",
       "       'I would like to learn English because I want to travel .',\n",
       "       'Just trying this website .', 'then I put them into the oven .',\n",
       "       'The weather is nice but we are planning to move out of this state .',\n",
       "       \"I 'm bad at English but I try the challenge  but I challenge myself until I feel I 've have enough .\",\n",
       "       'She envies me because in Bangkok I feel hot .'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual sentences\n",
    "train[\"correct\"].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediting results on cv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4632,
     "status": "ok",
     "timestamp": 1616919185508,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "tVQElX91q4yp",
    "outputId": "e6218d24-2fe1-4be8-ce35-bcccdcf2e478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It made my home since I can help .\n",
      "All the google we came back to the light on the child .\n",
      "Touring football .\n",
      "Anyway , the only the teacher for the best , I want to go to Ling   by TV program .\n",
      "And the number of being are conversations in the lunch .\n",
      "My name is Nakuro \n",
      "As my friends , too ,\n",
      "Recently I realized that the couple of this site .\n",
      "There are lots of clothes and short more .\n",
      "In the weather , to walk about what happened .\n"
     ]
    }
   ],
   "source": [
    "#predicted sentences\n",
    "for i in cv[\"incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1616919233522,
     "user": {
      "displayName": "satish agrawal",
      "photoUrl": "",
      "userId": "04473764286494136551"
     },
     "user_tz": -330
    },
    "id": "EaJlrCgj5XHi",
    "outputId": "4188e0d9-5336-46e7-940d-11a2705fc523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"It made me happy because I 'm a chocoholic .\",\n",
       "       \"All the things goes well before  o ' clock this night . tense\",\n",
       "       'Football Tournament',\n",
       "       'Anyway , the Beatles song that I like best is  The Long and Winding Road . ',\n",
       "       'And the number of believers are constantly on the rise .',\n",
       "       'My name is Jae yong han .', 'As my friends told me ,',\n",
       "       'Recently a few things have come to my attention .',\n",
       "       'There are lots of clothes and shoes in my room .',\n",
       "       'We talked about what we call happiness .'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual sentences\n",
    "cv[\"correct\"].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maybe it is completely .\n",
      "There are a lot of playing ball .\n",
      "that was a junior as a short .\n",
      "I 'm going to visit !\n",
      "Although the movie , the teacher at my Part , speaking , looks like Mr . . \n",
      "I know that I can not start my life because I have to be able to remember that he wants to understand what peo\n",
      "The content of the friends will be happy because of the other country .\n",
      "But I 'm interested in another picture .\n",
      "I have a lot of place to go to China to study english for the hotel  .\n",
      "Some class and term to go to the game , but they decided to study and participate .\n"
     ]
    }
   ],
   "source": [
    "#predicted sentences\n",
    "for i in test[\"incorrect\"].values[:10]:\n",
    "  print(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Maybe it was caused by pollen .',\n",
       "       'There are a lot of outlet plazas .',\n",
       "       'that was just an astonishing sight ,', 'I would never give up !',\n",
       "       'Although , my teacher said ,  Maria , please play more speedily .  ,',\n",
       "       'I know that I still can not relax my attention but I feel happy because we both are healthy now .',\n",
       "       'The contents were frozen foods which had been cooked by her .',\n",
       "       \"But I 'm trying to find important information . ? ?\",\n",
       "       'I plan to go to a junior high school to teach science on   th of Dec .',\n",
       "       'Some classmates got together , and decided to make a study group .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual sentences\n",
    "test[\"correct\"].values[:10]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y_cAes4-S4W2",
    "M9CskgglS4W4"
   ],
   "name": "GEC_EDA_Basic_Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
